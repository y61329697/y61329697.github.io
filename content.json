{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"404","date":"2023-01-21T08:06:47.000Z","updated":"2023-01-21T08:07:26.034Z","comments":true,"path":"404/index.html","permalink":"http://example.com/404/index.html","excerpt":"","text":""},{"title":"categories","date":"2023-01-21T06:58:32.000Z","updated":"2023-01-21T07:09:53.995Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2023-01-21T07:11:05.000Z","updated":"2023-01-21T07:13:19.579Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"你好"},{"title":"contact","date":"2023-01-21T07:14:46.000Z","updated":"2023-01-21T07:15:04.773Z","comments":true,"path":"contact/index.html","permalink":"http://example.com/contact/index.html","excerpt":"","text":""},{"title":"friends","date":"2023-01-21T07:15:17.000Z","updated":"2023-01-21T07:16:33.972Z","comments":true,"path":"friends/index.html","permalink":"http://example.com/friends/index.html","excerpt":"","text":"我的朋友啊，快来吧"},{"title":"tags","date":"2023-01-21T07:10:17.000Z","updated":"2023-01-21T07:10:43.582Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"神经网络入门基础","slug":"神经网络入门基础","date":"2023-02-10T01:25:00.000Z","updated":"2023-02-10T06:09:32.157Z","comments":true,"path":"2023/02/10/shen-jing-wang-luo-ru-men-ji-chu/","link":"","permalink":"http://example.com/2023/02/10/shen-jing-wang-luo-ru-men-ji-chu/","excerpt":"","text":"神经网络入门基础​ 神经网络中更新权值的的最成功算法: 误差逆传播（error BackPropagation，简称BP）算法。BP学习算法通常用在最为广泛使用的多层前馈神经网络中。 基本流程： 1.常见的神经网络模型1.1 受限玻尔兹曼机（RBM）​ 神经网络中有一类模型是为网络状态定义一个“能量”，能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数。Boltzmann（玻尔兹曼）机就是基于能量的模型，其神经元分为两层：显层和隐层。显层用于表示数据的输入和输出，隐层则被理解为数据的内在表达。Boltzmann机的神经元都是布尔型的，即只能取0、1值。标准的Boltzmann机是全连接的，也就是说各层内的神经元都是相互连接的，因此计算复杂度很高，而且难以用来解决实际问题。因此，我们经常使用一种特殊的Boltzmann机——受限玻尔兹曼机（Restricted Boltzmann Mechine，简称RBM），它层内无连接，层间有连接，可以看做是一个二部图。下图为Boltzmann机和RBM的结构示意图： 1.2 RBF网络​ RBF（Radial Basis Function）径向基函数网络是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合。下图为一个RBF神经网络示意图： 1.3 递归神经网络以及Elman网络​ 与前馈神经网络不同，递归神经网络（Recurrent Neural Networks，简称RNN）允许网络中出现环形结构，从而可以让一些神经元的输出反馈回来作为输入信号，这样的结构与信息反馈过程，使得网络在t时刻的输出状态不仅与t时刻的输入有关，还与t−1时刻的网络状态有关，从而能处理与时间有关的动态变化。 ​ Elman网络是最常用的递归神经网络之一，其结构如下图所示： ​ RNN一般的训练算法采用推广的BP算法。值得一提的是，RNN在（t+1）时刻网络的结果O(t+1)是该时刻输入和所有历史共同作用的结果，这样就达到了对时间序列建模的目的。因此，从某种意义上来讲，RNN被视为是时间深度上的深度学习也未尝不对。 ​ RNN在（t+1）时刻网络的结果O(t+1)是该时刻输入和所有历史共同作用的结果，这么讲其实也不是很准确，因为“梯度发散”同样也会发生在时间轴上，也就是说对于t时刻来说，它产生的梯度在时间轴上向历史传播几层之后就消失了，根本无法影响太遥远的过去。因此，“所有的历史”只是理想的情况。在实际中，这种影响也就只能维持若干个时间戳而已。换句话说，后面时间步的错误信号，往往并不能回到足够远的过去，像更早的时间步一样，去影响网络，这使它很难以学习远距离的影响。 为了解决上述时间轴上的梯度发散，机器学习领域发展出了长短时记忆单元（Long-Short Term Memory，简称LSTM），通过门的开关实现时间上的记忆功能，并防止梯度发散。其实除了学习历史信息，RNN和LSTM还可以被设计成为双向结构，即双向RNN、双向LSTM，同时利用历史和未来的信息。 2.深度学习​ 深度学习指的是深度神经网络模型，一般指网络层数在三层或者三层以上的神经网络结构。 理论上而言，参数越多的模型复杂度越高，“容量”也就越大，也就意味着它能完成更复杂的学习任务。就像前面多层感知机带给我们的启示一样，神经网络的层数直接决定了它对现实的刻画能力。但是在一般情况下，复杂模型的训练效率低，易陷入过拟合，因此难以受到人们的青睐。具体来讲就是，随着神经网络层数的加深，优化函数越来越容易陷入局部最优解（即过拟合，在训练样本上有很好的拟合效果，但是在测试集上效果很差）。同时，不可忽略的一个问题是随着网络层数增加，“梯度消失”（或者说是梯度发散diverge）现象更加严重。我们经常使用sigmoid函数作为隐含层的功能神经元，对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本接收不到有效的训练信号。 ​ 为了解决深层神经网络的训练问题，一种有效的手段是采取无监督逐层训练（unsupervised layer-wise training），其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，这被称之为“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）训练。比如Hinton在深度信念网络（Deep Belief Networks，简称DBN）中，每层都是一个RBM，即整个网络可以被视为是若干个RBM堆叠而成。在使用无监督训练时，首先训练第一层，这是关于训练样本的RBM模型，可按标准的RBM进行训练；然后，将第一层预训练号的隐节点视为第二层的输入节点，对第二层进行预训练；… 各层预训练完成后，再利用BP算法对整个网络进行训练。 ​ 事实上，“预训练+微调”的训练方式可被视为是将大量参数分组，对每组先找到局部看起来较好的设置，然后再基于这些局部较优的结果联合起来进行全局寻优。这样就在利用了模型大量参数所提供的自由度的同时，有效地节省了训练开销。 ​ 另一种节省训练开销的做法是进行“权共享”（weight sharing），即让一组神经元使用相同的连接权，这个策略在卷积神经网络（Convolutional Neural Networks，简称CNN）中发挥了重要作用。下图为一个CNN网络示意图： ​ CNN可以用BP算法进行训练，但是在训练中，无论是卷积层还是采样层，其每组神经元（即上图中的每一个“平面”）都是用相同的连接权，从而大幅减少了需要训练的参数数目。 3.反向传播参考链接（精品文章）：https://www.cnblogs.com/charlotte77/p/5629865.html ​ 这是典型的三层神经网络的基本构成，Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，我们现在手里有一堆数据{x1,x2,x3,…,xn},输出也是一堆数据{y1,y2,y3,…,yn},现在要他们在隐含层做某种变换，让你把数据灌进去后得到你期望的输出。如果你希望你的输出和原始输入一样，那么就是最常见的自编码模型（Auto-Encoder）。可能有人会问，为什么要输入输出都一样呢？有什么用啊？其实应用挺广的，在图像识别，文本分类等等都会用到，我会专门再写一篇Auto-Encoder的文章来说明，包括一些变种之类的。如果你的输出和原始输入不一样，那么就是很常见的人工神经网络了，相当于让原始数据通过一个映射来得到我们想要的输出数据，也就是我们今天要讲的话题。 ​ 第一层是输入层，包含两个神经元i1，i2，和截距项b1；第二层是隐含层，包含两个神经元h1,h2和截距项b2，第三层是输出o1,o2，每条线上标的wi是层与层之间连接的权重，激活函数我们默认为sigmoid函数。 现在对他们赋上初值，如下图（一个网络的初始状态）： 其中，输入数据 i1=0.05，i2=0.10; 输出数据 o1=0.01,o2=0.99; 初始权重 w1=0.15,w2=0.20,w3=0.25,w4=0.30; w5=0.40,w6=0.45,w7=0.50,w8=0.55 目标：给出输入数据i1,i2(0.05和0.10)，使输出尽可能与原始输出o1,o2(0.01和0.99)接近。 3.1 前向传播​ 简单来讲，就是线性运算和激活函数（sigmod）计算出输出结果output。 3.2 反向传播​ 反向传播其实就是梯度下降法，自变量是所有的神经网络中的权值参数。例如上图中的w1-w8。相当于存在一个函数是误差函数f（w1,w2…,w8）,所以求出每一个自变量的导数，然后使得总误差最小，梯度的反方向是下降最快的方向。 ​ 需要注意的是，神经元分为两个部分一个是网络的输入部分，一个是网络的激活函数部分。如下图所示，分别是neto1（直接输入的部分），还有经过激活函数的部分outo1。 ​ 同理，也要更新w1-w4，这个求导会更加复杂一点，因为层数更多了，然后更新w1，不断迭代优化参数。 3.3 下降分析​ 为什么在给定某些参数的时候，神经网络的损失函数不会下降。按理来讲，只要梯度足够小，是可以下降的。可以拟合任意的输入和输出，讲的特别好，只是我把输出看错了。 #coding:utf-8 import random import math # # 参数解释： # \"pd_\" ：偏导的前缀 # \"d_\" ：导数的前缀 # \"w_ho\" ：隐含层到输出层的权重系数索引 # \"w_ih\" ：输入层到隐含层的权重系数的索引 # 这是一个只有隐藏层和输出层的一个网络 # 为了确定反向传播，首先需要确定每一个w权值直接影响的神经元是哪一个 # 其实通过这个神经网络的图像就可以就可以推导出来导数的格式，就是多元函数求导的一种格式 # 这么看的话其实就和好理解如何求导了，需要用到那一个权值，这个更新是一轮全部，在更新过程中不改变w1-w8 # 问题的关键是要弄明白哪些数据是可以复用的 class NeuralNetwork: LEARNING_RATE = 0.5 def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_weights = None, output_layer_bias = None): self.num_inputs = num_inputs self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias) self.output_layer = NeuronLayer(num_outputs, output_layer_bias) # 初始化网络的权值 self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights) self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights) # python中的list无需声明可以添加任意类型的元素 # 权值其实是一个二维的矩阵，第一维是当前层神经元的个数，第二维是上一层神经元的个数 def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights): weight_num = 0 for h in range(len(self.hidden_layer.neurons)): for i in range(self.num_inputs): if not hidden_layer_weights: self.hidden_layer.neurons[h].weights.append(random.random()) else: self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num]) weight_num += 1 def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights): weight_num = 0 for o in range(len(self.output_layer.neurons)): for h in range(len(self.hidden_layer.neurons)): if not output_layer_weights: self.output_layer.neurons[o].weights.append(random.random()) else: self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num]) weight_num += 1 def inspect(self): print('------') print('* Inputs: {}'.format(self.num_inputs)) print('------') print('Hidden Layer') self.hidden_layer.inspect() print('------') print('* Output Layer') self.output_layer.inspect() print('------') # 前向传播 def feed_forward(self, inputs): hidden_layer_outputs = self.hidden_layer.feed_forward(inputs) return self.output_layer.feed_forward(hidden_layer_outputs) def train(self, training_inputs, training_outputs): self.feed_forward(training_inputs) # 1. 输出神经元的值 pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons) for o in range(len(self.output_layer.neurons)): # ∂E/∂zⱼ pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o]) # 2. 隐含层神经元的值 pd_errors_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons) for h in range(len(self.hidden_layer.neurons)): # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ d_error_wrt_hidden_neuron_output = 0 for o in range(len(self.output_layer.neurons)): d_error_wrt_hidden_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h] # ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂ pd_errors_wrt_hidden_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input() # 3. 更新输出层权重系数 for o in range(len(self.output_layer.neurons)): for w_ho in range(len(self.output_layer.neurons[o].weights)): # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho) # Δw = α * ∂Eⱼ/∂wᵢ self.output_layer.neurons[o].weights[w_ho] -= self.LEARNING_RATE * pd_error_wrt_weight # 4. 更新隐含层的权重系数 for h in range(len(self.hidden_layer.neurons)): for w_ih in range(len(self.hidden_layer.neurons[h].weights)): # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih) # Δw = α * ∂Eⱼ/∂wᵢ self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight def calculate_total_error(self, training_sets): total_error = 0 for t in range(len(training_sets)): training_inputs, training_outputs = training_sets[t] self.feed_forward(training_inputs) for o in range(len(training_outputs)): total_error += self.output_layer.neurons[o].calculate_error(training_outputs[o]) return total_error class NeuronLayer: def __init__(self, num_neurons, bias): # 同一层的神经元共享一个截距项b self.bias = bias if bias else random.random() self.neurons = [] for i in range(num_neurons): self.neurons.append(Neuron(self.bias)) def inspect(self): print('Neurons:', len(self.neurons)) for n in range(len(self.neurons)): print(' Neuron', n) for w in range(len(self.neurons[n].weights)): print(' Weight:', self.neurons[n].weights[w]) print(' Bias:', self.bias) # 前向传播，生成一个输出矩阵，前向传播依次遍历一层中的每一个神经元，把输入放到每一个神经元中。 def feed_forward(self, inputs): outputs = [] for neuron in self.neurons: outputs.append(neuron.calculate_output(inputs)) return outputs def get_outputs(self): outputs = [] for neuron in self.neurons: outputs.append(neuron.output) return outputs # 一个具体的神经元 class Neuron: def __init__(self, bias): self.bias = bias self.weights = [] # 每一个神经元对应于一个输入矩阵，对应于一个权值矩阵 def calculate_output(self, inputs): self.inputs = inputs self.output = self.squash(self.calculate_total_net_input()) return self.output def calculate_total_net_input(self): total = 0 for i in range(len(self.inputs)): total += self.inputs[i] * self.weights[i] return total + self.bias # 激活函数sigmoid def squash(self, total_net_input): return 1 / (1 + math.exp(-total_net_input)) # 这个可以封装为一个模块 def calculate_pd_error_wrt_total_net_input(self, target_output): return self.calculate_pd_error_wrt_output(target_output) * self.calculate_pd_total_net_input_wrt_input() # 每一个神经元的误差是由平方差公式计算的 def calculate_error(self, target_output): return 0.5 * (target_output - self.output) ** 2 def calculate_pd_error_wrt_output(self, target_output): return -(target_output - self.output) def calculate_pd_total_net_input_wrt_input(self): return self.output * (1 - self.output) def calculate_pd_total_net_input_wrt_weight(self, index): return self.inputs[index] # 文中的例子: # nn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], hidden_layer_bias=0.35, output_layer_weights=[0.4, 0.45, 0.5, 0.55], output_layer_bias=0.6) # nn.inspect() # for i in range(10000): # nn.train([0.05, 0.1], [0.9, 0.9]) # print(i, round(nn.calculate_total_error([[[0.05, 0.1], [0.9, 0.9]]]), 9)) # python的缩进 training_sets = [ [[0, 0], [0]], [[0, 1], [1]], [[1, 0], [1]], [[1, 1], [0]] ] nn = NeuralNetwork(len(training_sets[0][0]), 5, len(training_sets[0][1])) nn.inspect() for i in range(50000): training_inputs, training_outputs = random.choice(training_sets) nn.train(training_inputs, training_outputs) print(i, nn.calculate_total_error(training_sets)) # 拟合一个异或的神经网络 nn.feed_forward([0, 1]) print(nn.output_layer.get_outputs()) 4.卷积神经网络三个基本层： 4.1 卷积层​ 上文提到我们用传统的三层神经网络需要大量的参数，原因在于每个神经元都和相邻层的神经元相连接，但是思考一下，这种连接方式是必须的吗？全连接层的方式对于图像数据来说似乎显得不这么友好，因为图像本身具有“二维空间特征”，通俗点说就是局部特性。譬如我们看一张猫的图片，可能看到猫的眼镜或者嘴巴就知道这是张猫片，而不需要说每个部分都看完了才知道，啊，原来这个是猫啊。所以如果我们可以用某种方式对一张图片的某个典型特征识别，那么这张图片的类别也就知道了。这个时候就产生了卷积的概念。举个例子，现在有一个4*4的图像，我们设计两个卷积核，看看运用卷积核后图片会变成什么样。 ​ 由上图可以看到，原始图片是一张灰度图片,每个位置表示的是像素值，0表示白色，1表示黑色，（0，1）区间的数值表示灰色。对于这个4 * 4的图像，我们采用两个2 * 2的卷积核来计算。设定步长为1，即每次以2 * 2的固定窗口往右滑动一个单位。以第一个卷积核filter1为例，计算过程如下： 1 feature_map1(1,1) = 1*1 + 0*(-1) + 1*1 + 1*(-1) = 1 2 feature_map1(1,2) = 0*1 + 1*(-1) + 1*1 + 1*(-1) = -1 3 ``` 4 feature_map1(3,3) = 1*1 + 0*(-1) + 1*1 + 0*(-1) = 2 ​ feature_map代表经过卷积后的图像，feature_map尺寸计算公式：[ (原图片尺寸 -卷积核尺寸)/ 步长 ] + 1。 ​ 最顶层调用的函数是这样的，如下图所示，需要一些指定的参数： conv_pool_1 = paddle.networks.simple_img_conv_pool( input=img, filter_size=3, num_filters=2, num_channel=1, pool_stride=1, act=paddle.activation.Relu()) ​ 这是函数simple_img_conv_pool的定义： def simple_img_conv_pool(input, num_filters, filter_size, pool_size, pool_stride, act, pool_type='max', main_program=None, startup_program=None): conv_out = layers.conv2d( input=input, num_filters=num_filters, filter_size=filter_size, act=act, main_program=main_program, startup_program=startup_program) pool_out = layers.pool2d( input=conv_out, pool_size=pool_size, pool_type=pool_type, pool_stride=pool_stride, main_program=main_program, startup_program=startup_program) return pool_out ​ 可以看到这里面有两个输出，conv_out是卷积输出值，pool_out是池化输出值，最后只返回池化输出的值。conv_out和pool_out分别又调用了layers.py的conv2d和pool2d，去layers.py里我们可以看到conv2d和pool2d是如何实现的： ​ 可以发现，这conv2d使用到了help的函数来构建一些变量： def conv2d(input, num_filters, name=None, filter_size=[1, 1], act=None, groups=None, stride=[1, 1], padding=None, bias_attr=None, param_attr=None, main_program=None, startup_program=None): helper = LayerHelper('conv2d', **locals()) dtype = helper.input_dtype() num_channels = input.shape[1] if groups is None: num_filter_channels = num_channels else: if num_channels % groups is not 0: raise ValueError(\"num_channels must be divisible by groups.\") num_filter_channels = num_channels / groups if isinstance(filter_size, int): filter_size = [filter_size, filter_size] if isinstance(stride, int): stride = [stride, stride] if isinstance(padding, int): padding = [padding, padding] input_shape = input.shape filter_shape = [num_filters, num_filter_channels] + filter_size std = (2.0 / (filter_size[0]**2 * num_channels))**0.5 filter = helper.create_parameter( attr=helper.param_attr, shape=filter_shape, dtype=dtype, initializer=NormalInitializer(0.0, std, 0)) pre_bias = helper.create_tmp_variable(dtype) helper.append_op( type='conv2d', inputs={ 'Input': input, 'Filter': filter, }, outputs={\"Output\": pre_bias}, attrs={'strides': stride, 'paddings': padding, 'groups': groups}) pre_act = helper.append_bias_op(pre_bias, 1) return helper.append_activation(pre_act) ​ 以下是另一个函数pool2d： def pool2d(input, pool_size, pool_type, pool_stride=[1, 1], pool_padding=[0, 0], global_pooling=False, main_program=None, startup_program=None): if pool_type not in [\"max\", \"avg\"]: raise ValueError( \"Unknown pool_type: '%s'. It can only be 'max' or 'avg'.\", str(pool_type)) if isinstance(pool_size, int): pool_size = [pool_size, pool_size] if isinstance(pool_stride, int): pool_stride = [pool_stride, pool_stride] if isinstance(pool_padding, int): pool_padding = [pool_padding, pool_padding] helper = LayerHelper('pool2d', **locals()) dtype = helper.input_dtype() pool_out = helper.create_tmp_variable(dtype) helper.append_op( type=\"pool2d\", inputs={\"X\": input}, outputs={\"Out\": pool_out}, attrs={ \"poolingType\": pool_type, \"ksize\": pool_size, \"globalPooling\": global_pooling, \"strides\": pool_stride, \"paddings\": pool_padding }) return pool_out ​ 所以这个卷积过程就完成了。从上文的计算中我们可以看到，同一层的神经元可以共享卷积核，那么对于高位数据的处理将会变得非常简单。并且使用卷积核后图片的尺寸变小，方便后续计算，并且我们不需要手动去选取特征，只用设计好卷积核的尺寸，数量和滑动的步长就可以让它自己去训练了，省时又省力啊。 4.2 卷积核 ​ 那么问题来了，虽然我们知道了卷积核是如何计算的，但是为什么使用卷积核计算后分类效果要优于普通的神经网络呢？我们仔细来看一下上面计算的结果。通过第一个卷积核计算后的feature_map是一个三维数据，在第三列的绝对值最大，说明原始图片上对应的地方有一条垂直方向的特征，即像素数值变化较大；而通过第二个卷积核计算后，第三列的数值为0，第二行的数值绝对值最大，说明原始图片上对应的地方有一条水平方向的特征。 ​ 仔细思考一下，这个时候，我们设计的两个卷积核分别能够提取，或者说检测出原始图片的特定的特征。此时我们其实就可以把卷积核就理解为特征提取器啊！现在就明白了，为什么我们只需要把图片数据灌进去，设计好卷积核的尺寸、数量和滑动的步长就可以让自动提取出图片的某些特征，从而达到分类的效果啊！ 4.3 池化层​ 通过上一层22的卷积核操作后，我们将原始图像由44的尺寸变为了3 * 3的一个新的图片。池化层的主要目的是通过降采样的方式，在不影响图像质量的情况下，压缩图片，减少参数。简单来说，假设现在设定池化层采用MaxPooling，大小为2 * 2，步长为1，取每个窗口最大的数值重新，那么图片的尺寸就会由3 * 3变为2 * 2：(3-2)+1=2。从上例来看，会有如下变换： 采用max_pooling的原因： ​ 从计算方式来看，算是最简单的一种了，取max即可，但是这也引发一个思考，为什么需要Max Pooling，意义在哪里？如果我们只取最大值，那其他的值被舍弃难道就没有影响吗？不会损失这部分信息吗？如果认为这些信息是可损失的，那么是否意味着我们在进行卷积操作后仍然产生了一些不必要的冗余信息呢？ ​ 其实从上文分析卷积核为什么有效的原因来看，每一个卷积核可以看做一个特征提取器，不同的卷积核负责提取不同的特征，我们例子中设计的第一个卷积核能够提取出“垂直”方向的特征，第二个卷积核能够提取出“水平”方向的特征，那么我们对其进行Max Pooling操作后，提取出的是真正能够识别特征的数值，其余被舍弃的数值，对于我提取特定的特征并没有特别大的帮助。那么在进行后续计算使，减小了feature map的尺寸，从而减少参数，达到减小计算量，缺不损失效果的情况。 ​ 不过并不是所有情况Max Pooling的效果都很好，有时候有些周边信息也会对某个特定特征的识别产生一定效果，那么这个时候舍弃这部分“不重要”的信息，就不划算了。所以具体情况得具体分析，如果加了Max Pooling后效果反而变差了，不如把卷积后不加Max Pooling的结果与卷积后加了Max Pooling的结果输出对比一下，看看Max Pooling是否对卷积核提取特征起了反效果。 4.4 padding​ 所以到现在为止，我们的图片由4 * 4，通过卷积层变为3 * 3，再通过池化层变化2 * 2，如果我们再添加层，那么图片岂不是会越变越小？这个时候我们就会引出“Zero Padding”（补零），它可以帮助我们保证每次经过卷积或池化输出后图片的大小不变，如，上述例子我们如果加入Zero Padding，再采用3*3的卷积核，那么变换后的图片尺寸与原图片尺寸相同，如下图所示： ​ 通常情况下，我们希望图片做完卷积操作后保持图片大小不变，所以我们一般会选择尺寸为3 * 3的卷积核和1的zero padding，或者5 * 5的卷积核与2的zero padding，这样通过计算后，可以保留图片的原始尺寸。那么加入zero padding后的feature_map尺寸 =( width + 2 * padding_size - filter_size )/stride + 1。 4.5 Flatten层 &amp; Fully Connected Layer 到这一步，其实我们的一个完整的“卷积部分”就算完成了，如果想要叠加层数，一般也是叠加“Conv-MaxPooing”,通过不断的设计卷积核的尺寸，数量，提取更多的特征，最后识别不同类别的物体。做完Max Pooling后，我们就会把这些数据“拍平”，丢到Flatten层，然后把Flatten层的output放到full connected Layer里，采用softmax对其进行分类。 小结 这一节我们介绍了最基本的卷积神经网络的基本层的定义，计算方式和起的作用。有几个小问题可以供大家思考一下： 1.卷积核的尺寸必须为正方形吗？可以为长方形吗？如果是长方形应该怎么计算？ 2.卷积核的个数如何确定？每一层的卷积核的个数都是相同的吗？ 3.步长的向右和向下移动的幅度必须是一样的吗？ 如果对上面的讲解真的弄懂了的话，其实这几个问题并不难回答。下面给出我的想法，可以作为参考： 1.卷积核的尺寸不一定非得为正方形。长方形也可以，只不过通常情况下为正方形。如果要设置为长方形，那么首先得保证这层的输出形状是整数，不能是小数。如果你的图像是边长为 28 的正方形。那么卷积层的输出就满足 [ (28 - kernel_size)/ stride ] + 1 ，这个数值得是整数才行，否则没有物理意义。譬如，你算得一个边长为 3.6 的 feature map 是没有物理意义的。 pooling 层同理。FC 层的输出形状总是满足整数，其唯一的要求就是整个训练过程中 FC 层的输入得是定长的。如果你的图像不是正方形。那么在制作数据时，可以缩放到统一大小（非正方形），再使用非正方形的 kernel_size 来使得卷积层的输出依然是整数。总之，撇开网络结果设定的好坏不谈，其本质上就是在做算术应用题：如何使得各层的输出是整数。 2.由经验确定。通常情况下，靠近输入的卷积层，譬如第一层卷积层，会找出一些共性的特征，如手写数字识别中第一层我们设定卷积核个数为5个，一般是找出诸如”横线”、“竖线”、“斜线”等共性特征，我们称之为basic feature，经过max pooling后，在第二层卷积层，设定卷积核个数为20个，可以找出一些相对复杂的特征，如“横折”、“左半圆”、“右半圆”等特征，越往后，卷积核设定的数目越多，越能体现label的特征就越细致，就越容易分类出来，打个比方，如果你想分类出“0”的数字，你看到这个特征，能推测是什么数字呢？只有越往后，检测识别的特征越多，试过能识别这几个特征，那么我就能够确定这个数字是“0”。 3.有stride_w和stride_h，后者表示的就是上下步长。如果用stride，则表示stride_h=stride_w=stride。 5.CNN实现手写数字识别网络如下：注意CNN也有激活函数。 def convolutional_neural_network_org(img): # first conv layer conv_pool_1 = paddle.networks.simple_img_conv_pool( input=img, filter_size=3, num_filters=20, num_channel=1, pool_size=2, pool_stride=2, act=paddle.activation.Relu()) # second conv layer conv_pool_2 = paddle.networks.simple_img_conv_pool( input=conv_pool_1, filter_size=5, num_filters=50, num_channel=20, pool_size=2, pool_stride=2, act=paddle.activation.Relu()) # fully-connected layer predict = paddle.layer.fc( input=conv_pool_2, size=10, act=paddle.activation.Softmax()) return predict ​ 那么它的网络结构是：conv1—-&gt; conv2—-&gt;fully Connected layer。 非常简单的网络结构。第一层我们采取的是3 * 3的正方形卷积核，个数为20个，深度为1，stride为2，pooling尺寸为2 * 2，激活函数采取的为RELU；第二层只对卷积核的尺寸、个数和深度做了些变化，分别为5 * 5，50个和20；最后链接一层全连接，设定10个label作为输出，采用Softmax函数作为分类器，输出每个label的概率。 补充：关于通道的问题： ​ 为了更直观的理解，下面举个例子，卷积核的通道数核需要卷积的数据一致，最终输出的结果是4 * 4 * 1。 ​ 接下来，进行卷积操作，卷积核中的27个数字与分别与样本对应相乘后，再进行求和，得到第一个结果。依次进行，最终得到 4×4 的结果。 ​ 一般 channels 的含义是，每个卷积层中卷积核的数量。 5.1卷积神经网络的反向传播：​ 注意：池化层没有激活函数。 ​ 传统的神经网络是全连接形式的，如果进行反向传播，只需要由下一层对前一层不断的求偏导，即求链式偏导就可以求出每一层的误差敏感项，然后求出权重和偏置项的梯度，即可更新权重。而卷积神经网络有两个特殊的层：卷积层和池化层。池化层输出时不需要经过激活函数，是一个滑动窗口的最大值，一个常数，那么它的偏导是1。池化层相当于对上层图片做了一个压缩，这个反向求误差敏感项时与传统的反向传播方式不同。从卷积后的feature_map反向传播到前一层时，由于前向传播时是通过卷积核做卷积运算得到的feature_map，所以反向传播与传统的也不一样，需要更新卷积核的参数。下面我们介绍一下池化层和卷积层是如何做反向传播的。 5.1.1 卷积层实现反向传播​ 注意，net符号代表输入，out代表输出，旁别的角标代表哪一层。 ​ outi11就是i11层的输出，这里由i11来代替该符号。 感觉这个格式不太好看，建议直接参考原博客。 这个公式比较重要，是4 * 4转3 * 3的格式。 然后是关于误差对于上一层网络i的偏导数，这里的相乘是神经网络中的卷积。 ​ 关于卷积层实现反向传播，以下是我的理解： ​ 首先，最重要的是获得导数例如总误差对前面某一层的输入的导数，所以对于最终层（如果最终层是一个3 * 3的图像，而且通过这个层直接得到误差），我们可以看作是最终误差函数E=f(net11,net12…net33)，如果我们要对之前某一层i的参数求导，必然要知道E对i中的每一个输入的导数，所以我们需要用到反向传播，逐渐把问题向前推进，我们首先处理最后一层的上一层lnet，我们不妨设E=f（lnet11…lnet44），要求得E对lnetij的导数，我们需要知道，lnetij和netij的关系，例如下一个图： ​ 这时，我们就可以使用多元函数的导数的公式来获得上一层lnetij的导数，最终的格式是这样，同时需要注意激活函数的导数： ​ 所以，我们就得到了lnet和E之间的导数关系，进而，我们就可以求得lnet上一层llnet与E的关系，同理，就是构造一个类似的矩阵，只不过h矩阵变为了上一层的h矩阵（注意这里的h矩阵都是经过180度变换的），delta矩阵也是lnet层的举证，同时需要根据需求实现zero-padding。 5.1.2 池化层反向传播 池化层的反向传播就比较好求了，看着下面的图，左边是上一层的输出，也就是卷积层的输出feature_map，右边是池化层的输入，还是先根据前向传播，把式子都写出来，方便计算： ​ 假设上一层这个滑动窗口的最大值是outi11。 5.2 手写CNN网络​ 详情请见博客https://www.cnblogs.com/charlotte77/p/7783261.html。 ​","categories":[{"name":"神经网络","slug":"神经网络","permalink":"http://example.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"Typora","slug":"Typora","permalink":"http://example.com/tags/Typora/"},{"name":"神经网络","slug":"神经网络","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}],"author":"y61329697"},{"title":"Hello World","slug":"hello-world","date":"2023-01-21T03:22:45.612Z","updated":"2023-01-21T03:22:45.612Z","comments":true,"path":"2023/01/21/hello-world/","link":"","permalink":"http://example.com/2023/01/21/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"typora-vue-theme主题介绍","slug":"test","date":"2013-01-17T01:25:00.000Z","updated":"2023-02-10T06:10:32.563Z","comments":true,"path":"2013/01/17/test/","link":"","permalink":"http://example.com/2013/01/17/test/","excerpt":"","text":"这是一篇文章#include&lt;bits/stdc++.h&gt; #define INF 100//INF为比图中任何权值都大的数 #define maxSize 7 //图的顶点数 #define number 12 //图的边数 using namespace std; typedef struct {//图的定义 int edges[maxSize][maxSize];//邻接矩阵的定义 int n, e; //分别为顶点数和边数 }MGraph; /*迪杰斯特拉算法代码，函数结束时dist[]存放了v点到其余各顶点的最短路径长度，path[]中保存从V到各顶点的最短路径*/ #define MAXSIZE 20 #define PLACENUM 12 #define INF 9999 // 此处定义999为无穷大 struct { int vexnum,arcnum; //节点数和边数 int vexs[MAXSIZE]; // 节点名 int arcs[MAXSIZE][MAXSIZE]; //俩个节点之间的值 } net; /*补充的结构体net,2019.7.3*/ void Dijkstra(int x,int y) // x为源点，y为终点 { int i,j,k; int min; int u; //下一个放入集合p的点 int dis[net.vexnum]; // 最短路径 int mark[net.vexnum]; // 被mark的便是已经遍历,未被mark的便是未遍历 /*首先进行最短路径初始化*/ for(i=0; i&lt;net.vexnum; i++) { mark[i] = 0; dis[i] = net.arcs[x][i]; } mark[x]=1; // 标记源点 for(k=0; k&lt;net.vexnum; k++) // for 大循环 { min = INF; // min初始化最大值，便于后来数据替换（每一个点的出度入度判断） /*寻找遍历到点联通路径（与之相连线的点）中权值最小的一条； 标记遍历点；*/ for(i=0; i&lt;net.vexnum; i++) { if(mark[i]==0&amp;&amp;min&gt;dis[i]) //判断未遍历点 且 被赋值的最短路径（dis[i]&lt;INF），未被赋值的点 // 应当min==dis[i]=INF { min = dis[i]; //在已知赋值最短路径中，寻找权值最小的点并将他作为下一个遍历 u=i; //点u点 } } mark[u]=1; //标记u点，下面u修正将会以最短路径进行辐射 /*修正最短路径*/ for(i=0;i&lt;net.vexnum;i++) { if(!mark[i]&amp;&amp;dis[i]&gt;dis[u]+net.arcs[u][i]) // ！mark[i]判断不去走回头路， /*dis[i]&gt;dis[u]+net.arcs[u][i]有俩个用途：①若u链接的是x源点没有赋值最短路径的点，那么这里可以赋值②若是赋值过的点，那么可以判断是上一个dis[i]（此时是被赋值过的）是不是真正的最短路径，即修正。*/ { dis[i] = dis[u] + net.arcs[u][i]; //若A-&gt;C比A-&gt;B-&gt;C更长那么A-&gt;B-&gt;C则是到C的最短路径，下图将解释。 } } } printf(\"最短路径值为： %d\",dis[y]); }","categories":[{"name":"Markdown","slug":"Markdown","permalink":"http://example.com/categories/Markdown/"}],"tags":[{"name":"Typora","slug":"Typora","permalink":"http://example.com/tags/Typora/"},{"name":"Markdown","slug":"Markdown","permalink":"http://example.com/tags/Markdown/"}],"author":"y61329697"}],"categories":[{"name":"神经网络","slug":"神经网络","permalink":"http://example.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"Markdown","slug":"Markdown","permalink":"http://example.com/categories/Markdown/"}],"tags":[{"name":"Typora","slug":"Typora","permalink":"http://example.com/tags/Typora/"},{"name":"神经网络","slug":"神经网络","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"Markdown","slug":"Markdown","permalink":"http://example.com/tags/Markdown/"}]}